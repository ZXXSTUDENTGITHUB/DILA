import torch
import torch.nn.functional as F
from torch_geometric.utils import dense_to_sparse
from torch_geometric.nn import GCNConv, GATConv
import os
import numpy as np


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class base(GATConv):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(base, self).__init__()

        self.conv1 = GATConv(input_dim, hidden_dim)

        self.conv2 = GATConv(hidden_dim, hidden_dim)

        self.conv3 = GATConv(hidden_dim, int(output_dim))

    def forward(self, x, edge_index, train_idx=None):
        x = F.relu(self.conv1(x, edge_index))

        x = F.relu(self.conv2(x, edge_index))

        x = self.conv3(x, edge_index)

        if train_idx is not None:
            x_train = x[train_idx]
            return F.log_softmax(x_train, dim=1)
        else:
            return F.log_softmax(x, dim=1)


def load_XA(dataname, datadir="XAL"):
    prefix = os.path.join(datadir, dataname)
    filename_A = prefix + "_A.npy"
    filename_X = prefix + "_X.npy"
    A = np.load(filename_A)
    X = np.load(filename_X)
    return A, X


def load_labels(dataname, datadir="XAL"):
    prefix = os.path.join(datadir, dataname)
    filename_L = prefix + "_L.npy"
    L = np.load(filename_L)
    return L


def load_ckpt(dataname, datadir="XAL", isbest=False):
    """Load a pre-trained pytorch model from checkpoint."""
    filename = create_filename(datadir, dataname, isbest)
    print(filename)
    if os.path.isfile(filename):
        print("=> loading checkpoint '{}'".format(filename))
        ckpt = torch.load(filename, map_location=torch.device("cpu"))
    else:
        print("Checkpoint does not exist!")
        print("Checked path -- {}".format(filename))
        print("Make sure you have provided the correct path!")
        print("You may have forgotten to train a model for this dataset.")
        print()
        print("To train one of the paper's models, run the following")
        print(">> python train.py --dataset=DATASET_NAME")
        print()
        raise Exception("File not found.")
    return ckpt


def create_filename(save_dir, dataname, isbest=False, num_epochs=-1):
    filename = os.path.join(save_dir, dataname)

    if isbest:
        filename = os.path.join(filename, "best")
    elif num_epochs > 0:
        filename = os.path.join(filename, str(num_epochs))
    return filename + ".pth.tar"


dataset = "syn1"
dataset_path = "dataset/"

A_np, X_np = load_XA(dataset, datadir=dataset_path)
num_nodes = X_np.shape[0]
labels = load_labels(dataset, datadir=dataset_path)
labels = torch.tensor(labels, dtype=torch.long).to(device)
num_class = max(labels) + 1


A = torch.tensor(A_np, dtype=torch.float32).to(device)
X = torch.tensor(X_np, dtype=torch.float32).to(device)

if dataset == "syn3":
    one_hot_tensor = (
        F.one_hot(torch.sum(A, 1).type(torch.LongTensor)).type(torch.float32).to(device)
    )
    X = torch.cat([one_hot_tensor, X], 1)
else:
    X = F.one_hot(torch.sum(A, 1).type(torch.LongTensor)).type(torch.float32).to(device)

input_dim = X.shape[1]
edge_index, _ = dense_to_sparse(A)
edge_index = edge_index.to(device)

gnn_model = base(input_dim=input_dim, hidden_dim=64, output_dim=num_class).to(device)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)

train_idx = [
    269,
    461,
    337,
    466,
    618,
    575,
    636,
    645,
    431,
    602,
    82,
    239,
    64,
    100,
    603,
    435,
    698,
    85,
    79,
    669,
    224,
    440,
    516,
    322,
    27,
    463,
    134,
    271,
    219,
    351,
    280,
    103,
    321,
    582,
    696,
    621,
    547,
    58,
    246,
    192,
    113,
    451,
    344,
    74,
    437,
    650,
    354,
    477,
    648,
    90,
    514,
    614,
    485,
    121,
    164,
    651,
    12,
    613,
    596,
    685,
    513,
    639,
    585,
    112,
    545,
    310,
    1,
    492,
    327,
    488,
    229,
    591,
    626,
    521,
    629,
    295,
    667,
    569,
    405,
    490,
    638,
    481,
    37,
    190,
    168,
    679,
    261,
    273,
    179,
    506,
    567,
    404,
    53,
    81,
    167,
    422,
    332,
    31,
    420,
    394,
    77,
    287,
    660,
    446,
    401,
    616,
    309,
    381,
    583,
    526,
    212,
    304,
    423,
    153,
    215,
    388,
    432,
    641,
    317,
    11,
    589,
    41,
    628,
    72,
    243,
    505,
    690,
    528,
    54,
    627,
    228,
    370,
    36,
    67,
    444,
    22,
    622,
    453,
    250,
    642,
    114,
    6,
    145,
    336,
    604,
    456,
    267,
    34,
    88,
    178,
    220,
    286,
    342,
    640,
    597,
    551,
    18,
    360,
    202,
    69,
    319,
    238,
    282,
    222,
    555,
    231,
    509,
    643,
    61,
    469,
    474,
    590,
    258,
    105,
    612,
    504,
    457,
    290,
    644,
    270,
    495,
    615,
    659,
    556,
    391,
    380,
    204,
    372,
    161,
    165,
    657,
    73,
    5,
    289,
    541,
    277,
    211,
    666,
    550,
    392,
    486,
    606,
    483,
    374,
    350,
    325,
    363,
    634,
    180,
    691,
    436,
    257,
    24,
    40,
    159,
    260,
    137,
    531,
    511,
    294,
    357,
    684,
    441,
    263,
    537,
    623,
    387,
    2,
    15,
    523,
    499,
    208,
    91,
    598,
    455,
    665,
    406,
    293,
    99,
    361,
    308,
    605,
    21,
    155,
    369,
    382,
    200,
    491,
    30,
    84,
    68,
    130,
    465,
    400,
    147,
    326,
    26,
    527,
    419,
    407,
    341,
    343,
    425,
    65,
    95,
    253,
    237,
    572,
    195,
    87,
    671,
    241,
    338,
    529,
    373,
    553,
    38,
    390,
    552,
    439,
    198,
    609,
    144,
    86,
    414,
    464,
    131,
    445,
    415,
    379,
    592,
    221,
    515,
    376,
    255,
    19,
    536,
    176,
    563,
    52,
    593,
    160,
    284,
    214,
    402,
    544,
    427,
    8,
    418,
    240,
    520,
    681,
    29,
    188,
    333,
    182,
    438,
    358,
    45,
    123,
    480,
    424,
    125,
    216,
    148,
    411,
    316,
    664,
    235,
    70,
    635,
    356,
    458,
    210,
    694,
    484,
    162,
    364,
    106,
    158,
    557,
    473,
    236,
    320,
    564,
    17,
    632,
    315,
    264,
    97,
    140,
    386,
    501,
    329,
    429,
    251,
    620,
    218,
    102,
    426,
    245,
    191,
    265,
    579,
    92,
    266,
    479,
    658,
    695,
    318,
    631,
    482,
    573,
    396,
    497,
    328,
    574,
    680,
    377,
    149,
    150,
    278,
    588,
    462,
    542,
    118,
    249,
    610,
    630,
    413,
    46,
    524,
    385,
    292,
    562,
    330,
    301,
    226,
    693,
    637,
    470,
    126,
    305,
    50,
    35,
    433,
    172,
    306,
    232,
    549,
    55,
    655,
    272,
    339,
    539,
    543,
    532,
    173,
    421,
    115,
    442,
    107,
    581,
    209,
    194,
    43,
    510,
    104,
    223,
    116,
    500,
    48,
    530,
    166,
    233,
    365,
    259,
    398,
    283,
    525,
    577,
    611,
    78,
    57,
    63,
    408,
    170,
    443,
    141,
    475,
    558,
    599,
    76,
    393,
    135,
    311,
    101,
    697,
    334,
    348,
    450,
    580,
    507,
    682,
    656,
    468,
    247,
    323,
    183,
    496,
    538,
    428,
    403,
    430,
    256,
    171,
    186,
    227,
    300,
    652,
    275,
    274,
    608,
    252,
    80,
    285,
    489,
    94,
    244,
    494,
    649,
    607,
    410,
    207,
    368,
    646,
    409,
    234,
    9,
    156,
    472,
    110,
    687,
    677,
    355,
    647,
    349,
    683,
    654,
    3,
    668,
    353,
    302,
    163,
    225,
    584,
    533,
    197,
    378,
    66,
    109,
    28,
    587,
    146,
    367,
    561,
    206,
    331,
    276,
    262,
    699,
    184,
    56,
    142,
    181,
    519,
    395,
    169,
    674,
    96,
    518,
    546,
    447,
    673,
    371,
    566,
    661,
    47,
    128,
    594,
    487,
    75,
    42,
    279,
    122,
    535,
    688,
    570,
    203,
    625,
]
num_epochs = 100
for epoch in range(num_epochs):
    gnn_model.train()
    optimizer.zero_grad()
    pred = gnn_model(X, edge_index, train_idx)
    loss = criterion(pred, labels[train_idx])
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}")

save_path = "trained_gat"
save_filename = "syn1.pth.tar"
save_dict = {
    "epoch": 100,
    "model_type": base,
    "optimizer": optimizer,
    "model_state": gnn_model.state_dict(),
    "optimizer_state": optimizer.state_dict(),
    "save_data": {
        "adj": A_np,
        "feat": X_np,
        "label": labels.cpu().numpy(),
        "pred": pred.detach().cpu().numpy(),
        "train_idx": train_idx,
    },
}

torch.save(save_dict, os.path.join(save_path, save_filename))
